import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras import layers
import numpy as np
import json
import time
import re
import random
import os
import faiss
from huggingface_hub import hf_hub_download
from tokenizers import Tokenizer

# === Custom layers ===
class L2NormLayer(layers.Layer):
    def __init__(self, axis=1, epsilon=1e-10, **kwargs):
        super().__init__(**kwargs)
        self.axis = axis
        self.epsilon = epsilon
    def call(self, inputs):
        return tf.math.l2_normalize(inputs, axis=self.axis, epsilon=self.epsilon)
    def get_config(self):
        return {"axis": self.axis, "epsilon": self.epsilon, **super().get_config()}

class LearnableWeightedPooling(layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.dense = None  # ì´ˆê¸°ì—” Noneìœ¼ë¡œ ì„ ì–¸í•´ë‘ 

    def build(self, input_shape):
        # input_shape: (batch_size, seq_len, embed_dim)
        self.dense = layers.Dense(1, use_bias=False)
        self.dense.build(input_shape)  # Dense ë ˆì´ì–´ build í˜¸ì¶œí•´ì„œ ê°€ì¤‘ì¹˜ ìƒì„±
        self.built = True  # build ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •

    def call(self, inputs, mask=None):
        scores = self.dense(inputs)  # (batch, seq_len, 1)

        if mask is not None:
            mask = tf.cast(mask, scores.dtype)
            minus_inf = -1e9
            scores = scores + (1 - mask[..., tf.newaxis]) * minus_inf

        weights = tf.nn.softmax(scores, axis=1)  # (batch, seq_len, 1)
        weighted_sum = tf.reduce_sum(inputs * weights, axis=1)  # (batch, embed_dim)
        return weighted_sum
    

# === í™˜ê²½ ë³€ìˆ˜ ë° í† í° ===
os.environ["HF_HOME"] = "/tmp/hf_cache"
hf_token = os.getenv("HF_TOKEN")

# === ìƒìˆ˜ ===
MAX_SEQ_LEN = 256
BATCH_SIZE = 1024
SIM_THRESHOLD = 0.0  # í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥

# === ëª¨ë¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ===
TK_MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM-4", filename="ko_bpe.json", repo_type="model", token=hf_token)
MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM-4", filename="sentence_encoder_model.keras", repo_type="model", token=hf_token)
JSONL_PATH = hf_hub_download(repo_id="Yuchan5386/Kode-2", filename="filtered_conversations.jsonl", repo_type="dataset", token=hf_token)
EMBEDDINGS_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM-4", filename="answer_embeddings_streaming.npz", repo_type="model", token=hf_token)
INDEX_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM-4", filename="answer_faiss_index.index", repo_type="model", token=hf_token)

# tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = Tokenizer.from_file(TK_MODEL_PATH)
# === ì¸ì½”ë” ë¡œë“œ ===
encoder = load_model(MODEL_PATH, custom_objects={"L2NormLayer": L2NormLayer, "LearnableWeightedPooling": LearnableWeightedPooling})

# === ë‹µë³€ í…ìŠ¤íŠ¸ ë¡œë“œ ===
def load_answers_from_jsonl(jsonl_path):
    answers = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                obj = json.loads(line)
                convs = obj.get("conversations", [])
                for turn in convs:
                    if turn.get("from") == "gpt" and "value" in turn:
                        answers.append(turn["value"])
            except json.JSONDecodeError:
                continue
    return answers

answer_texts = load_answers_from_jsonl(JSONL_PATH)

# === ì„ë² ë”© ë¡œë“œ ===
embedding_npz = np.load(EMBEDDINGS_PATH)
answer_embs = embedding_npz['embeddings'].astype('float32')

# === faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë¡œë“œ/ìƒì„± ===
dim = answer_embs.shape[1]
if os.path.isfile(INDEX_PATH):
    faiss_index = faiss.read_index(INDEX_PATH)
else:
    faiss_index = faiss.IndexFlatIP(dim)      # FAISS ì¸ë±ìŠ¤ë¥¼ faiss_index ë³€ìˆ˜ì— ë‹´ìŠµë‹ˆë‹¤.
    faiss_index.add(answer_embs)
    faiss.write_index(faiss_index, INDEX_PATH)


def tk_tokenize(texts):
    # textsê°€ ë‹¨ì¼ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
    if isinstance(texts, str):
        texts = [texts]
    encoded = [tokenizer.encode(text).ids for text in texts]
    padded = tf.keras.preprocessing.sequence.pad_sequences(
        encoded, maxlen=MAX_SEQ_LEN, padding='post', truncating='post'
    )
    return padded


# === ì„ë² ë”© ìƒì„± ===
def encode_sentences(texts):
    seqs = tk_tokenize(texts)
    dataset = tf.data.Dataset.from_tensor_slices(seqs).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return encoder.predict(dataset, verbose=0).astype('float32')

def remove_invalid_unicode(text):
    return re.sub(r'[\ud800-\udfff]', '', text)



FILTERED_SENTENCES = [
    "sklearn.preprocessingì—ì„œ StandardScalerë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.sklearn.decompositionì—ì„œ PCAë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.íŒŒì´í”„ë¼ì¸ = íŒŒì´í”„ë¼ì¸(ë‹¨ê³„=[    ('ìŠ¤ì¼€ì¼ëŸ¬', StandardScaler()),    ('pca', PCA())])",
    """def create_sentence(words):    sentence = ""    missing_words = []    for word in words:        if word.endswith("."):            sentence += word        else:            sentence += word + " "    if len(missing_words) > 0:        print("ë‹¤ìŒ ë‹¨ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: ")        for word in missing_words:            missing_word = input(f"{word}ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")            sentence = sentence.replace(f"{word} ", f"{missing_word} ")    ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:```pythonwords = ["ì´ê²ƒ", "ì´ê²ƒ", ... ]"""
]

def filter_response(text):
    for f_text in FILTERED_SENTENCES:
        if f_text in text:
            return ""
    return text

QUERY_EXPANSION_DICT = {
    "tensorflow": "import tensorflow as tf",
    "keras": "import tensorflow.keras as keras",
    "numpy": "import numpy as np",
    "pandas": "import pandas as pd",
    "sklearn": "import sklearn",
    "knn": "sklearn knn knn knn Knn KNN",
    "KNN": "sklearn knn knn knn Knn KNN"
}

def expand_query(text):
    return QUERY_EXPANSION_DICT.get(text.strip().lower(), text)

def generate_response(user_input, top_k=3, similarity_threshold=SIM_THRESHOLD):
    user_input = expand_query(user_input)
    augmented = user_input
    query_emb = encode_sentences([augmented])
    D, I = faiss_index.search(query_emb, top_k)  # index â†’ faiss_index ë¡œ ë³€ê²½
    sims = D[0]
    best_idx = I[0][0]
    best_score = sims[0]

    if best_score < similarity_threshold:
        return random.choice([
            "ìŒ... ì´ê±´ ê·¸ëƒ¥ ë„˜ê¸¸ê²Œìš”!", "ê·¸ê±´ ì˜ ëª¨ë¥´ê² ì–´ìš”. ğŸ˜…",
            "ì´ê±´ ë‚´ ì§€ì‹ ë°–ì¸ ê²ƒ ê°™ì•„ìš”.", "íŒ¨ìŠ¤! ë‹¤ë¥¸ ì§ˆë¬¸ ê°€ë³´ìê³ ~"
        ])

    best_response = answer_texts[best_idx]

    return filter_response(best_response)